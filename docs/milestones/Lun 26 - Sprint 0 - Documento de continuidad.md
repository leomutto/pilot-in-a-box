
# üìò Sprint 0 ‚Äì Documento de continuidad

**Proyecto:** MASS Simple / Enterprise v1.1  
**Rol de este documento:** Punto de restauraci√≥n y contrato de contexto para futuras sesiones.

---

## 1. Punto de partida del Sprint 0

Al inicio de este Sprint 0, el estado era:

- **Backend FastAPI** existente, pero:
  - Sin flujo de autenticaci√≥n consolidado.
  - Sin protecci√≥n consistente de endpoints.
  - Sin tabla `users` formalizada en la base.
- **MASS**:
  - Modelo conceptual definido (MASS Simple v1.0 / Enterprise v1.1).
  - Persistencia en base no alineada con el modelo final.
- **Base de datos PostgreSQL**:
  - Tablas creadas manualmente (no versionadas con Alembic).
  - `mass_requests` exist√≠a en una versi√≥n previa, sin `schema_version`, `idempotency_key`, ni FK a `users`.
- **Alembic**:
  - Configurado, pero:
    - Migraciones no aplicadas correctamente.
    - Drift entre lo que hab√≠a en la base y lo que describ√≠an las migraciones.
- **Docker**:
  - Im√°genes cacheadas.
  - C√≥digo dentro del contenedor desfasado respecto al host.
- **Frontend (localhost:3000)**:
  - Interfaz b√°sica operativa.
  - Conexi√≥n al backend, pero sin autenticaci√≥n integrada ni validada end-to-end.

Objetivo del Sprint 0 (expl√≠cito o impl√≠cito):

- **Normalizar la base de datos y las migraciones.**
- **Consolidar autenticaci√≥n y protecci√≥n de endpoints.**
- **Alinear MASS con Enterprise v1.1.**
- **Dejar backend y base listos para Sprint 1 (funcionalidad de negocio).**

---

## 2. Trabajo realizado en backend (arquitectura y seguridad)

### 2.1. Autenticaci√≥n y HTTP Bearer

Se trabaj√≥ en:

- Implementar y/o consolidar **autenticaci√≥n basada en JWT** usando esquema `HTTPBearer`.
- Centralizar la validaci√≥n del token:
  - Extracci√≥n del token desde el header `Authorization: Bearer <token>`.
  - Validaci√≥n de firma y expiraci√≥n.
  - Decodificaci√≥n de claims (incluyendo `user_id`).
- Integrar la autenticaci√≥n con la capa de dependencias de FastAPI:
  - Dependencias reutilizables para:
    - Obtener el usuario autenticado.
    - Proteger endpoints sensibles.
- Asegurar que:
  - Endpoints cr√≠ticos (MASS, datos de usuario, etc.) **no sean accesibles sin token v√°lido**.
  - El flujo de error sea consistente (401/403 seg√∫n corresponda).

**Resultado:**  
El backend ahora tiene un esquema de autenticaci√≥n basado en HTTP Bearer, con JWT validado de forma centralizada y listo para ser usado por el frontend.

---

### 2.2. Protecci√≥n de endpoints

Se revisaron y ajustaron:

- Endpoints p√∫blicos vs. privados.
- Uso de dependencias de seguridad en FastAPI:
  - Endpoints MASS protegidos.
  - Endpoints de administraci√≥n o lectura protegidos.
- Se evit√≥:
  - Exponer endpoints sensibles sin autenticaci√≥n.
  - Dejar rutas de prueba sin control.

**Resultado:**  
Los endpoints relevantes de MASS y usuarios est√°n protegidos por el esquema HTTP Bearer, alineados con el modelo de seguridad esperado.

---

### 2.3. Archivos y modelos MASS (`mass_request`, `mass_payload`)

Se trabaj√≥ en:

- Definir y/o refinar los modelos de dominio para MASS:
  - **`MassRequest`** (o equivalente) como entidad principal.
  - **`MassPayload`** (o equivalente) para representar el contenido JSON asociado.
- Alinear estos modelos con:
  - La estructura final de la tabla `mass_requests`.
  - El contrato MASS Simple v1.0 / Enterprise v1.1.
- Asegurar que:
  - El payload se persista como `JSON`.
  - Existan campos para:
    - `schema_version`
    - `correlation_id`
    - `idempotency_key`
    - `user_id`
    - `payload_json`
    - `created_at`

**Resultado:**  
El backend tiene modelos y archivos coherentes con la tabla `mass_requests` reconstruida, listos para soportar el flujo MASS end-to-end.

---

## 3. Trabajo realizado en base de datos y migraciones

### 3.1. Problemas detectados

Durante el Sprint 0 se detectaron:

- Alembic ejecutando migraciones con c√≥digo viejo dentro del contenedor.
- Migraci√≥n `create_users_table` usando `op.execute()` con par√°metros ‚Üí error `TypeError: execute() takes 2 positional arguments but 3 were given`.
- Tabla `mass_requests` ya existente cuando Alembic intentaba crearla ‚Üí `DuplicateTable`.
- Migraci√≥n de rebuild (`rebuild_mass_requests_20260126`) intentando crear √≠ndices que ya exist√≠an ‚Üí `DuplicateTable` sobre `ix_mass_requests_correlation_id`.
- Drift entre:
  - Archivos de migraci√≥n en el host.
  - Archivos de migraci√≥n dentro del contenedor.
  - Estado real de la base.

---

### 3.2. Correcciones clave en migraciones

#### 3.2.1. Migraci√≥n `923738512075_create_users_table`

- Se corrigi√≥ el uso de `op.execute()`:
  - Se reemplaz√≥ la versi√≥n con `sa.text(...), params` por una versi√≥n con SQL literal interpolado.
- Se asegur√≥ que:
  - La tabla `users` tenga:
    - `id` (PK, serial)
    - `email` (unique, not null)
    - `hashed_password` (not null)
    - `created_at` (default `now()`)
- Se cre√≥ un **usuario seed** con contrase√±a hasheada usando `argon2`.

**Estado final de `users`:**

```sql
Table "public.users"
 id              | integer                  | not null | nextval('users_id_seq'::regclass)
 email           | character varying        | not null
 hashed_password | character varying        | not null
 created_at      | timestamp with time zone |          | now()
Indexes:
 users_pkey PRIMARY KEY (id)
 users_email_key UNIQUE (email)
Referenced by:
 mass_requests.user_id_fkey
```

---

#### 3.2.2. Migraci√≥n `1facca6dc8e8_create_mass_requests_table`

- Se us√≥ como base inicial para crear `mass_requests`.
- Luego fue ‚Äúsuperada‚Äù por la migraci√≥n de rebuild.

---

#### 3.2.3. Migraci√≥n `rebuild_mass_requests_20260126`

Problemas detectados:

- `sa.Column(..., index=True)` en `correlation_id` e `idempotency_key`.
- Adem√°s, `op.create_index(...)` expl√≠cito para los mismos campos.
- Resultado: intento de crear dos veces el mismo √≠ndice ‚Üí `DuplicateTable`.

Correcci√≥n conceptual (aunque al final la migraci√≥n termin√≥ ejecut√°ndose bien tras dropear tablas):

- La forma correcta es:
  - **No usar `index=True` en las columnas**.
  - Crear √≠ndices expl√≠citos con `op.create_index(...)`.

**Estado final de `mass_requests`:**

```sql
Table "public.mass_requests"
 id              | integer                  | not null | nextval('mass_requests_id_seq'::regclass)
 schema_version  | character varying        | not null
 correlation_id  | character varying        | not null
 idempotency_key | character varying        | not null
 user_id         | integer                  | not null
 payload_json    | json                     | not null
 created_at      | timestamp with time zone | not null | now()
Indexes:
 ix_mass_requests_correlation_id
 ix_mass_requests_idempotency_key
Foreign keys:
 mass_requests_user_id_fkey ‚Üí users(id)
```

---

### 3.3. Secuencia final aplicada

1. Drop controlado de tablas:
   - `DROP TABLE IF EXISTS mass_requests CASCADE;`
   - `DROP TABLE IF EXISTS users CASCADE;`
2. Ejecuci√≥n de migraciones:

   ```bash
   alembic upgrade head
   ```

   Resultado:

   - `1facca6dc8e8` ‚Üí crea `mass_requests` inicial.
   - `923738512075` ‚Üí crea `users` + seed user.
   - `rebuild_mass_requests_20260126` ‚Üí dropea y reconstruye `mass_requests` alineada con Enterprise v1.1.

3. Verificaci√≥n final:

   ```sql
   SELECT * FROM alembic_version;
   -- devuelve: rebuild_mass_requests_20260126
   ```

**Conclusi√≥n:**  
La base est√° **totalmente sincronizada** con Alembic y alineada con el modelo MASS Enterprise v1.1.

---

## 4. Trabajo realizado en Docker y entorno

- Se detect√≥ que el contenedor backend estaba usando migraciones viejas.
- Se reconstruy√≥ la imagen del backend con:

  ```bash
  docker compose build --no-cache backend
  docker compose up -d
  ```

- Se verific√≥ dentro del contenedor:

  ```bash
  cat /app/migrations/versions/923738512075_create_users_table.py
  cat /app/migrations/versions/2026_01_26_2300_rebuild_mass_requests.py
  ```

- Se confirm√≥ que:
  - Los archivos dentro del contenedor coinciden con los del host.
  - Alembic ejecuta la versi√≥n correcta de las migraciones.

**Resultado:**  
Se elimin√≥ el drift entre host y contenedor.  
El entorno Docker ahora es confiable como referencia de verdad.

---

## 5. Pruebas realizadas en backend y frontend

### 5.1. Backend (localhost:8000)

Se realizaron pruebas (manuales y/o con herramientas tipo curl/Postman) para:

- Verificar que el backend levanta sin errores.
- Probar endpoints protegidos con HTTP Bearer:
  - Acceso sin token ‚Üí rechazo (401/403).
  - Acceso con token v√°lido ‚Üí √©xito.
- Confirmar que:
  - El flujo de autenticaci√≥n funciona.
  - El usuario seed puede autenticarse.
  - Los endpoints MASS responden correctamente (en la medida en que ya est√°n implementados).

### 5.2. Frontend (localhost:3000)

Se realizaron pruebas de:

- Conexi√≥n del frontend al backend.
- Flujo de login (en la medida en que el frontend ya lo soporta).
- Comportamiento de la UI frente a respuestas del backend:
  - Errores de autenticaci√≥n.
  - Respuestas exitosas.

**Nota importante:**  
El foco del Sprint 0 estuvo m√°s en **infraestructura, base de datos y seguridad** que en UX o l√≥gica de negocio del frontend.  
Pero se valid√≥ que el frontend puede hablar con el backend en el entorno Docker actual.

---

## 6. Estado actual del sistema (foto final del Sprint 0)

- **Backend:**
  - Autenticaci√≥n HTTP Bearer con JWT.
  - Endpoints protegidos.
  - Modelos MASS (`mass_request`, `mass_payload`) alineados con la base.
- **Base de datos:**
  - `users` y `mass_requests` en estado final correcto.
  - `alembic_version` en `rebuild_mass_requests_20260126`.
- **Migraciones:**
  - Cadena completa aplicada sin errores.
  - Migraciones corregidas y sincronizadas con el contenedor.
- **Docker:**
  - Im√°genes reconstruidas sin cache.
  - C√≥digo dentro del contenedor alineado con el host.
- **Frontend:**
  - Conectado al backend.
  - Capaz de interactuar con endpoints (seg√∫n implementaci√≥n actual).

Este es el **punto de restauraci√≥n oficial** del Sprint 0.

---

## 7. Pr√≥ximos pasos l√≥gicos y t√©cnicos para cerrar Sprint 0 y abrir Sprint 1

Estos son los pasos que, si me tra√©s este documento en una pr√≥xima sesi√≥n, yo voy a asumir como **pendientes inmediatos**:

### 7.1. Validaci√≥n funcional end-to-end

1. **Login completo:**
   - Confirmar login con el usuario seed desde el frontend.
   - Verificar que el token JWT se almacena y se usa en requests posteriores.

2. **MASS end-to-end:**
   - Enviar un MASS request real desde el frontend o Postman.
   - Confirmar inserci√≥n en `mass_requests`.
   - Verificar que `schema_version`, `correlation_id`, `idempotency_key`, `user_id` y `payload_json` se registran correctamente.

### 7.2. Endpoints MASS (Sprint 1)

- Definir y consolidar:
  - `POST /mass-requests`
  - `GET /mass-requests/{id}`
  - `GET /mass-requests` (listado / filtros por `correlation_id`, etc.)
- Asegurar:
  - Validaci√≥n de payload.
  - Manejo de errores consistente.
  - Logs estructurados.

### 7.3. Documentaci√≥n t√©cnica

Crear o completar:

- **‚ÄúArquitectura de Datos ‚Äì MASS v1.1‚Äù**
- **‚ÄúGu√≠a de Migraciones Alembic (MASS)‚Äù**
- **‚ÄúFlujo de Autenticaci√≥n y Protecci√≥n de Endpoints‚Äù**

---

## 8. C√≥mo usar este documento en la pr√≥xima sesi√≥n

Cuando me pegues este documento y me digas:  
> ‚ÄúEste es el Sprint 0 ‚Äì Documento de continuidad‚Äù

Yo voy a saber, sin reinterpretar nada, que:

- La base est√° en el estado exacto que se describe ac√°.
- Las migraciones est√°n aplicadas hasta `rebuild_mass_requests_20260126`.
- `users` y `mass_requests` tienen la estructura final indicada.
- El backend tiene autenticaci√≥n HTTP Bearer y endpoints protegidos.
- MASS est√° alineado con Enterprise v1.1 a nivel de datos.
- Docker est√° sincronizado.
- El foco siguiente es:
  - Validaci√≥n end-to-end.
  - Endpoints MASS.
  - Documentaci√≥n y Sprint 1.

---
